# -*- coding: utf-8 -*-
"""DLP_project_LSTM+BERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X-Bzb_Y4g9pZ_y6ZQ8VOqCgnJqjibmFZ
"""

!pip install kaggle yfinance tensorflow transformers pandas_datareader ta

"""COMPARISON MODELS PT 01:

"""

# Configure GPU memory growth before any TensorFlow imports
import tensorflow as tf
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(f"GPU configuration error: {e}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, precision_recall_curve
from sklearn.ensemble import RandomForestClassifier
import joblib
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, SimpleRNN, Concatenate, Dropout, Lambda, LayerNormalization
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from transformers import AutoTokenizer, TFAutoModel
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.layers import Layer
from tensorflow.keras.callbacks import Callback
import os
import shutil
from google.colab import drive
import time

# Mount Google Drive
drive.mount('/content/drive')
drive_model_path = '/content/drive/My Drive/StockPrediction_Model'
if not os.path.exists(drive_model_path):
    os.makedirs(drive_model_path)

# Custom Attention Layer
class ScaledDotProductAttention(Layer):
    def __init__(self, **kwargs):
        super(ScaledDotProductAttention, self).__init__(**kwargs)

    def build(self, input_shape):
        super(ScaledDotProductAttention, self).build(input_shape)

    def call(self, inputs):
        query, key, value = inputs
        d_k = tf.cast(tf.shape(key)[-1], tf.float32)
        scores = tf.matmul(query, key, transpose_b=True) / tf.sqrt(d_k)
        weights = tf.nn.softmax(scores, axis=-1)
        output = tf.matmul(weights, value)
        return output

# Custom Learning Rate Scheduler with Warmup
class WarmUpScheduler(Callback):
    def __init__(self, warmup_epochs, initial_lr, final_lr):
        super(WarmUpScheduler, self).__init__()
        self.warmup_epochs = warmup_epochs
        self.initial_lr = initial_lr
        self.final_lr = final_lr

    def on_epoch_begin(self, epoch, logs=None):
        if epoch < self.warmup_epochs:
            lr = self.initial_lr + (self.final_lr - self.initial_lr) * (epoch / self.warmup_epochs)
            self.model.optimizer.learning_rate.assign(lr)

# Load and preprocess data
combined_df = pd.read_csv('Combined_News_DJIA.csv', parse_dates=['Date'])
djia_df = pd.read_csv('upload_DJIA_table.csv', parse_dates=['Date'])
reddit_df = pd.read_csv('RedditNews.csv', parse_dates=['Date'])

full_df = pd.merge(combined_df, djia_df, on='Date', how='inner')

# Technical indicators
def add_technical_indicators(df):
    df['Log_Return'] = np.log(df['Adj Close'] / df['Adj Close'].shift(1))
    df['MA_10'] = df['Adj Close'].rolling(window=10).mean()
    df['MA_20'] = df['Adj Close'].rolling(window=20).mean()
    df['MA_50'] = df['Adj Close'].rolling(window=50).mean()
    df['MA_200'] = df['Adj Close'].rolling(window=200).mean()
    df['Volatility_20'] = df['Adj Close'].rolling(window=20).std()
    df['Volatility_50'] = df['Adj Close'].rolling(window=50).std()
    delta = df['Adj Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df['RSI'] = 100 - (100 / (1 + rs))
    exp12 = df['Adj Close'].ewm(span=12, adjust=False).mean()
    exp26 = df['Adj Close'].ewm(span=26, adjust=False).mean()
    df['MACD'] = exp12 - exp26
    df['Signal_Line'] = df['MACD'].ewm(span=9, adjust=False).mean()
    df['MACD_Hist'] = df['MACD'] - df['Signal_Line']
    df['Upper_Band'] = df['MA_20'] + (2 * df['Volatility_20'])
    df['Lower_Band'] = df['MA_20'] - (2 * df['Volatility_20'])
    df['Band_Width'] = df['Upper_Band'] - df['Lower_Band']
    low_14 = df['Low'].rolling(window=14).min()
    high_14 = df['High'].rolling(window=14).max()
    df['Stochastic_K'] = 100 * ((df['Close'] - low_14) / (high_14 - low_14))
    df['Stochastic_D'] = df['Stochastic_K'].rolling(window=3).mean()
    return df.dropna()

full_df = add_technical_indicators(full_df)

# Sentiment analysis
def get_sentiment_scores(texts, tokenizer, model, batch_size=16):
    sentiment_scores = []
    classification_head = Dense(3, activation='softmax', name='sentiment_classifier')
    for i in range(0, len(texts), batch_size):
        tf.keras.backend.clear_session()
        batch_texts = texts[i:i + batch_size]
        encodings = tokenizer(batch_texts, truncation=True, padding='max_length', max_length=128, return_tensors='tf')
        outputs = model(encodings)
        cls_embeddings = outputs.last_hidden_state[:, 0, :]
        logits = classification_head(cls_embeddings)
        probs = tf.nn.softmax(logits, axis=-1)
        sentiment_scores.append(probs.numpy())
    return np.vstack(sentiment_scores)

full_df['Combined_News'] = full_df.filter(regex='Top\d+').apply(
    lambda x: ' '.join(x.str.lower().replace(r'[^\w\s]', '', regex=True).replace(r'\d+', '', regex=True)), axis=1)

tokenizer = AutoTokenizer.from_pretrained("yiyanghkust/finbert-tone")
finbert = TFAutoModel.from_pretrained("yiyanghkust/finbert-tone")
sentiment_scores = get_sentiment_scores(full_df['Combined_News'].tolist(), tokenizer, finbert)
full_df['Sentiment_Positive'] = sentiment_scores[:, 0]
full_df['Sentiment_Negative'] = sentiment_scores[:, 1]
full_df['Sentiment_Neutral'] = sentiment_scores[:, 2]

# Save preprocessed data
full_df.to_csv('preprocessed_data.csv')
shutil.copy('preprocessed_data.csv', os.path.join(drive_model_path, 'preprocessed_data.csv'))

# Train-test split
train_df = full_df[full_df['Date'] < '2015-01-01']
test_df = full_df[full_df['Date'] >= '2015-01-01']

num_features = ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close',
                'Log_Return', 'MA_10', 'MA_20', 'MA_50', 'MA_200',
                'Volatility_20', 'Volatility_50', 'RSI',
                'MACD', 'Signal_Line', 'MACD_Hist',
                'Upper_Band', 'Lower_Band', 'Band_Width',
                'Stochastic_K', 'Stochastic_D',
                'Sentiment_Positive', 'Sentiment_Negative', 'Sentiment_Neutral']

train_mean = train_df[num_features].mean()
train_std = train_df[num_features].std()
train_num = (train_df[num_features] - train_mean) / train_std
test_num = (test_df[num_features] - train_mean) / train_std

# Data augmentation
def augment_numerical_data(data, n_samples, noise_factor=0.1):
    data = np.array(data)
    std = np.std(data, axis=0)
    synthetic_data = np.zeros((n_samples, data.shape[1]))
    indices = np.random.choice(data.shape[0], size=n_samples, replace=True)
    for i in range(n_samples):
        noise = np.random.normal(0, noise_factor * std, size=data.shape[1])
        synthetic_data[i] = data[indices[i]] + noise
    return synthetic_data, indices

n_samples = len(train_num)
synthetic_data, selected_indices = augment_numerical_data(train_num.values, n_samples=n_samples)
augmented_train_num = np.vstack([train_num.values, synthetic_data])
train_texts = train_df['Combined_News'].tolist()
augmented_train_texts = train_texts + [train_texts[i] for i in selected_indices]
augmented_y_train = np.hstack([train_df['Label'].values, train_df['Label'].iloc[selected_indices].values])

# Sequence creation
window_size = 20
def create_sequences(data, labels, texts, tokenizer, window_size=20):
    sequences = []
    label_seq = []
    text_seq = []
    for i in range(len(data) - window_size):
        sequences.append(data[i:i+window_size])
        label_seq.append(labels[i+window_size])
        text_seq.append(texts[i+window_size])
    sequences = np.array(sequences)
    label_seq = np.array(label_seq)
    encodings = tokenizer(text_seq, truncation=True, padding='max_length', max_length=128, return_tensors='tf')
    return sequences, label_seq, encodings

X_train_num, y_train, train_encodings = create_sequences(augmented_train_num, augmented_y_train, augmented_train_texts, tokenizer, window_size)
X_test_num, y_test, test_encodings = create_sequences(test_num.values, test_df['Label'].values, test_df['Combined_News'].tolist(), tokenizer, window_size)

# Validate sample sizes
n_train_samples = X_train_num.shape[0]
if not (n_train_samples == train_encodings['input_ids'].shape[0] == train_encodings['attention_mask'].shape[0] == y_train.shape[0]):
    raise ValueError(f"Training data cardinality mismatch")
n_test_samples = X_test_num.shape[0]
if not (n_test_samples == test_encodings['input_ids'].shape[0] == test_encodings['attention_mask'].shape[0] == y_test.shape[0]):
    raise ValueError(f"Test data cardinality mismatch")

# --- LSTM+FinBERT Model ---
def build_hybrid_model(num_features_count, bert_model):
    num_input = Input(shape=(window_size, num_features_count), name='num_input')
    num_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(num_input)
    num_norm = LayerNormalization()(num_lstm)
    num_att = ScaledDotProductAttention()([num_norm, num_norm, num_norm])
    num_lstm2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(num_att)
    num_dense = Dense(128, activation='relu')(num_lstm2)
    input_ids = Input(shape=(128,), dtype=tf.int32, name='input_ids')
    attention_mask = Input(shape=(128,), dtype=tf.int32, name='attention_mask')
    def call_bert(inputs):
        input_ids, attention_mask = inputs
        outputs = bert_model({'input_ids': input_ids, 'attention_mask': attention_mask})
        return outputs.last_hidden_state[:, 0, :]
    bert_output = Lambda(call_bert, output_shape=(768,))([input_ids, attention_mask])
    text_dense = Dense(128, activation='relu')(bert_output)
    combined = Concatenate()([num_dense, text_dense])
    combined = Dropout(0.6)(combined)
    combined = Dense(128, activation='relu')(combined)
    combined = Dropout(0.4)(combined)
    combined = Dense(64, activation='relu')(combined)
    output = Dense(1, activation='sigmoid')(combined)
    model = Model(inputs=[num_input, input_ids, attention_mask], outputs=output)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
                  loss='binary_crossentropy',
                  metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])
    return model

lstm_model = build_hybrid_model(len(num_features), finbert)
lstm_model.summary()

class_counts = np.bincount(y_train.astype(int))
class_weights = {0: len(y_train) / (2 * class_counts[0]), 1: len(y_train) / (2 * class_counts[1])}

callbacks = [
    EarlyStopping(monitor='val_auc', patience=15, mode='max', restore_best_weights=True),
    ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=5, min_lr=1e-6),
    ModelCheckpoint('best_model.keras', monitor='val_auc', save_best_only=True, mode='max'),
    WarmUpScheduler(warmup_epochs=5, initial_lr=1e-5, final_lr=1e-4)
]

start_time = time.time()
history_lstm = lstm_model.fit(
    {'num_input': X_train_num, 'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']},
    y_train,
    validation_data=({'num_input': X_test_num, 'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']}, y_test),
    epochs=30,
    batch_size=64,
    callbacks=callbacks,
    class_weight=class_weights
)
lstm_training_time = time.time() - start_time

lstm_model.save('final_model.keras')
shutil.copy('best_model.keras', os.path.join(drive_model_path, 'best_model.keras'))
shutil.copy('final_model.keras', os.path.join(drive_model_path, 'final_model.keras'))

y_pred_lstm = lstm_model.predict(
    {'num_input': X_test_num, 'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']}
).flatten()
lstm_report = classification_report(y_test, (y_pred_lstm > 0.5).astype(int), output_dict=True)
lstm_auc = roc_auc_score(y_test, y_pred_lstm)

# --- Random Forest Model ---
# Flatten sequences for Random Forest
X_train_rf = X_train_num.reshape(X_train_num.shape[0], -1)
X_test_rf = X_test_num.reshape(X_test_num.shape[0], -1)

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
start_time = time.time()
rf_model.fit(X_train_rf, y_train)
rf_training_time = time.time() - start_time

joblib.dump(rf_model, 'rf_model.pkl')
shutil.copy('rf_model.pkl', os.path.join(drive_model_path, 'rf_model.pkl'))

y_pred_rf = rf_model.predict_proba(X_test_rf)[:, 1]
rf_report = classification_report(y_test, (y_pred_rf > 0.5).astype(int), output_dict=True)
rf_auc = roc_auc_score(y_test, y_pred_rf)

# --- RNN Model ---
def build_rnn_model(num_features_count, bert_model):
    num_input = Input(shape=(window_size, num_features_count), name='num_input')
    num_rnn = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(128, return_sequences=True))(num_input)
    num_norm = LayerNormalization()(num_rnn)
    num_att = ScaledDotProductAttention()([num_norm, num_norm, num_norm])
    num_rnn2 = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(64))(num_att)
    num_dense = Dense(128, activation='relu')(num_rnn2)
    input_ids = Input(shape=(128,), dtype=tf.int32, name='input_ids')
    attention_mask = Input(shape=(128,), dtype=tf.int32, name='attention_mask')
    def call_bert(inputs):
        input_ids, attention_mask = inputs
        outputs = bert_model({'input_ids': input_ids, 'attention_mask': attention_mask})
        return outputs.last_hidden_state[:, 0, :]
    bert_output = Lambda(call_bert, output_shape=(768,))([input_ids, attention_mask])
    text_dense = Dense(128, activation='relu')(bert_output)
    combined = Concatenate()([num_dense, text_dense])
    combined = Dropout(0.6)(combined)
    combined = Dense(128, activation='relu')(combined)
    combined = Dropout(0.4)(combined)
    combined = Dense(64, activation='relu')(combined)
    output = Dense(1, activation='sigmoid')(combined)
    model = Model(inputs=[num_input, input_ids, attention_mask], outputs=output)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
                  loss='binary_crossentropy',
                  metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])
    return model

rnn_model = build_rnn_model(len(num_features), finbert)
rnn_model.summary()

callbacks_rnn = [
    EarlyStopping(monitor='val_auc', patience=15, mode='max', restore_best_weights=True),
    ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=5, min_lr=1e-6),
    ModelCheckpoint('rnn_model.keras', monitor='val_auc', save_best_only=True, mode='max'),
    WarmUpScheduler(warmup_epochs=5, initial_lr=1e-5, final_lr=1e-4)
]

start_time = time.time()
history_rnn = rnn_model.fit(
    {'num_input': X_train_num, 'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']},
    y_train,
    validation_data=({'num_input': X_test_num, 'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']}, y_test),
    epochs=30,
    batch_size=64,
    callbacks=callbacks_rnn,
    class_weight=class_weights
)
rnn_training_time = time.time() - start_time

rnn_model.save('rnn_final_model.keras')
shutil.copy('rnn_model.keras', os.path.join(drive_model_path, 'rnn_model.keras'))
shutil.copy('rnn_final_model.keras', os.path.join(drive_model_path, 'rnn_final_model.keras'))

y_pred_rnn = rnn_model.predict(
    {'num_input': X_test_num, 'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']}
).flatten()
rnn_report = classification_report(y_test, (y_pred_rnn > 0.5).astype(int), output_dict=True)
rnn_auc = roc_auc_score(y_test, y_pred_rnn)

# Model comparison
comparison_data = {
    'Model': ['LSTM+FinBERT', 'Random Forest', 'RNN'],
    'Accuracy': [lstm_report['accuracy'], rf_report['accuracy'], rnn_report['accuracy']],
    'AUC': [lstm_auc, rf_auc, rnn_auc],
    'F1-Score (Class 1)': [lstm_report['1']['f1-score'], rf_report['1']['f1-score'], rnn_report['1']['f1-score']],
    'Training Time (s)': [lstm_training_time, rf_training_time, rnn_training_time]
}
comparison_df = pd.DataFrame(comparison_data)
comparison_df.to_csv('model_comparison.csv')
shutil.copy('model_comparison.csv', os.path.join(drive_model_path, 'model_comparison.csv'))

print("Model Comparison:")
print(comparison_df)

# Save test data for dashboard
np.save('X_test_num.npy', X_test_num)
np.save('y_test.npy', y_test)
np.save('test_encodings_input_ids.npy', test_encodings['input_ids'].numpy())
np.save('test_encodings_attention_mask.npy', test_encodings['attention_mask'].numpy())
shutil.copy('X_test_num.npy', os.path.join(drive_model_path, 'X_test_num.npy'))
shutil.copy('y_test.npy', os.path.join(drive_model_path, 'y_test.npy'))
shutil.copy('test_encodings_input_ids.npy', os.path.join(drive_model_path, 'test_encodings_input_ids.npy'))
shutil.copy('test_encodings_attention_mask.npy', os.path.join(drive_model_path, 'test_encodings_attention_mask.npy'))

print(f"All models and data saved to Google Drive at: {drive_model_path}")

!ls '/content/drive/My Drive/StockPrediction_Model'

# Generate a requirements.txt with exact versions
!pip freeze > colab_requirements.txt

# Download the file to your local machine
from google.colab import files
files.download('colab_requirements.txt')

"""FIXING THE KERAS ISSUE FOR COMAPRISON AND DASHBOARD

"""

# Configure GPU memory growth before any TensorFlow imports
import tensorflow as tf
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(f"GPU configuration error: {e}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, precision_recall_curve
from sklearn.ensemble import RandomForestClassifier
import joblib
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, SimpleRNN, Concatenate, Dropout, Lambda
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from transformers import AutoTokenizer, TFAutoModel
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.layers import Layer
from tensorflow.keras.callbacks import Callback
import os
import shutil
from google.colab import drive
import time

# Mount Google Drive
drive.mount('/content/drive')
drive_model_path = '/content/drive/My Drive/StockPrediction_Model'
if not os.path.exists(drive_model_path):
    os.makedirs(drive_model_path)

# Set mixed precision policy to float32 to match local environment
tf.keras.mixed_precision.set_global_policy('float32')

# Custom Attention Layer
class ScaledDotProductAttention(Layer):
    def __init__(self, **kwargs):
        super(ScaledDotProductAttention, self).__init__(**kwargs)
    def build(self, input_shape):
        super(ScaledDotProductAttention, self).build(input_shape)
    def call(self, inputs):
        query, key, value = inputs
        d_k = tf.cast(tf.shape(key)[-1], tf.float32)
        scores = tf.matmul(query, key, transpose_b=True) / tf.sqrt(d_k)
        weights = tf.nn.softmax(scores, axis=-1)
        output = tf.matmul(weights, value)
        return output

# Custom Learning Rate Scheduler with Warmup
class WarmUpScheduler(Callback):
    def __init__(self, warmup_epochs, initial_lr, final_lr):
        super(WarmUpScheduler, self).__init__()
        self.warmup_epochs = warmup_epochs
        self.initial_lr = initial_lr
        self.final_lr = final_lr
    def on_epoch_begin(self, epoch, logs=None):
        if epoch < self.warmup_epochs:
            lr = self.initial_lr + (self.final_lr - self.initial_lr) * (epoch / self.warmup_epochs)
            self.model.optimizer.learning_rate.assign(lr)

# Load and preprocess data
combined_df = pd.read_csv('Combined_News_DJIA.csv', parse_dates=['Date'])
djia_df = pd.read_csv('upload_DJIA_table.csv', parse_dates=['Date'])
reddit_df = pd.read_csv('RedditNews.csv', parse_dates=['Date'])
full_df = pd.merge(combined_df, djia_df, on='Date', how='inner')

def add_technical_indicators(df):
    df['Log_Return'] = np.log(df['Adj Close'] / df['Adj Close'].shift(1))
    df['MA_10'] = df['Adj Close'].rolling(window=10).mean()
    df['MA_20'] = df['Adj Close'].rolling(window=20).mean()
    df['MA_50'] = df['Adj Close'].rolling(window=50).mean()
    df['MA_200'] = df['Adj Close'].rolling(window=200).mean()
    df['Volatility_20'] = df['Adj Close'].rolling(window=20).std()
    df['Volatility_50'] = df['Adj Close'].rolling(window=50).std()
    delta = df['Adj Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df['RSI'] = 100 - (100 / (1 + rs))
    exp12 = df['Adj Close'].ewm(span=12, adjust=False).mean()
    exp26 = df['Adj Close'].ewm(span=26, adjust=False).mean()
    df['MACD'] = exp12 - exp26
    df['Signal_Line'] = df['MACD'].ewm(span=9, adjust=False).mean()
    df['MACD_Hist'] = df['MACD'] - df['Signal_Line']
    df['Upper_Band'] = df['MA_20'] + (2 * df['Volatility_20'])
    df['Lower_Band'] = df['MA_20'] - (2 * df['Volatility_20'])
    df['Band_Width'] = df['Upper_Band'] - df['Lower_Band']
    low_14 = df['Low'].rolling(window=14).min()
    high_14 = df['High'].rolling(window=14).max()
    df['Stochastic_K'] = 100 * ((df['Close'] - low_14) / (high_14 - low_14))
    df['Stochastic_D'] = df['Stochastic_K'].rolling(window=3).mean()
    return df.dropna()

full_df = add_technical_indicators(full_df)

def get_sentiment_scores(texts, tokenizer, model, batch_size=16):
    sentiment_scores = []
    classification_head = Dense(3, activation='softmax', name='sentiment_classifier')
    for i in range(0, len(texts), batch_size):
        tf.keras.backend.clear_session()
        batch_texts = texts[i:i + batch_size]
        encodings = tokenizer(batch_texts, truncation=True, padding='max_length', max_length=128, return_tensors='tf')
        outputs = model(encodings)
        cls_embeddings = outputs.last_hidden_state[:, 0, :]
        logits = classification_head(cls_embeddings)
        probs = tf.nn.softmax(logits, axis=-1)
        sentiment_scores.append(probs.numpy())
    return np.vstack(sentiment_scores)

full_df['Combined_News'] = full_df.filter(regex='Top\d+').apply(
    lambda x: ' '.join(x.str.lower().replace(r'[^\w\s]', '', regex=True).replace(r'\d+', '', regex=True)), axis=1)
tokenizer = AutoTokenizer.from_pretrained("yiyanghkust/finbert-tone")
finbert = TFAutoModel.from_pretrained("yiyanghkust/finbert-tone")
sentiment_scores = get_sentiment_scores(full_df['Combined_News'].tolist(), tokenizer, finbert)
full_df['Sentiment_Positive'] = sentiment_scores[:, 0]
full_df['Sentiment_Negative'] = sentiment_scores[:, 1]
full_df['Sentiment_Neutral'] = sentiment_scores[:, 2]

full_df.to_csv('preprocessed_data.csv')
shutil.copy('preprocessed_data.csv', os.path.join(drive_model_path, 'preprocessed_data.csv'))

train_df = full_df[full_df['Date'] < '2015-01-01']
test_df = full_df[full_df['Date'] >= '2015-01-01']

num_features = ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close',
                'Log_Return', 'MA_10', 'MA_20', 'MA_50', 'MA_200',
                'Volatility_20', 'Volatility_50', 'RSI',
                'MACD', 'Signal_Line', 'MACD_Hist',
                'Upper_Band', 'Lower_Band', 'Band_Width',
                'Stochastic_K', 'Stochastic_D',
                'Sentiment_Positive', 'Sentiment_Negative', 'Sentiment_Neutral']

train_mean = train_df[num_features].mean()
train_std = train_df[num_features].std()
train_num = (train_df[num_features] - train_mean) / train_std
test_num = (test_df[num_features] - train_mean) / train_std

def augment_numerical_data(data, n_samples, noise_factor=0.1):
    data = np.array(data)
    std = np.std(data, axis=0)
    synthetic_data = np.zeros((n_samples, data.shape[1]))
    indices = np.random.choice(data.shape[0], size=n_samples, replace=True)
    for i in range(n_samples):
        noise = np.random.normal(0, noise_factor * std, size=data.shape[1])
        synthetic_data[i] = data[indices[i]] + noise
    return synthetic_data, indices

n_samples = len(train_num)
synthetic_data, selected_indices = augment_numerical_data(train_num.values, n_samples=n_samples)
augmented_train_num = np.vstack([train_num.values, synthetic_data])
train_texts = train_df['Combined_News'].tolist()
augmented_train_texts = train_texts + [train_texts[i] for i in selected_indices]
augmented_y_train = np.hstack([train_df['Label'].values, train_df['Label'].iloc[selected_indices].values])

window_size = 20
def create_sequences(data, labels, texts, tokenizer, window_size=20):
    sequences = []
    label_seq = []
    text_seq = []
    for i in range(len(data) - window_size):
        sequences.append(data[i:i+window_size])
        label_seq.append(labels[i+window_size])
        text_seq.append(texts[i+window_size])
    sequences = np.array(sequences)
    label_seq = np.array(label_seq)
    encodings = tokenizer(text_seq, truncation=True, padding='max_length', max_length=128, return_tensors='tf')
    return sequences, label_seq, encodings

X_train_num, y_train, train_encodings = create_sequences(augmented_train_num, augmented_y_train, augmented_train_texts, tokenizer, window_size)
X_test_num, y_test, test_encodings = create_sequences(test_num.values, test_df['Label'].values, test_df['Combined_News'].tolist(), tokenizer, window_size)

n_train_samples = X_train_num.shape[0]
if not (n_train_samples == train_encodings['input_ids'].shape[0] == train_encodings['attention_mask'].shape[0] == y_train.shape[0]):
    raise ValueError(f"Training data cardinality mismatch")
n_test_samples = X_test_num.shape[0]
if not (n_test_samples == test_encodings['input_ids'].shape[0] == test_encodings['attention_mask'].shape[0] == y_test.shape[0]):
    raise ValueError(f"Test data cardinality mismatch")

def build_hybrid_model(num_features_count, bert_model):
    num_input = Input(shape=(window_size, num_features_count), name='num_input', dtype='float32')
    num_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(num_input)
    num_norm = tf.keras.layers.BatchNormalization()(num_lstm)  # Replaced LayerNormalization with BatchNormalization
    num_att = ScaledDotProductAttention()([num_norm, num_norm, num_norm])
    num_lstm2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(num_att)
    num_dense = Dense(128, activation='relu')(num_lstm2)
    input_ids = Input(shape=(128,), dtype=tf.int32, name='input_ids')
    attention_mask = Input(shape=(128,), dtype=tf.int32, name='attention_mask')
    def call_bert(inputs):
        input_ids, attention_mask = inputs
        outputs = bert_model({'input_ids': input_ids, 'attention_mask': attention_mask})
        return outputs.last_hidden_state[:, 0, :]
    bert_output = Lambda(call_bert, output_shape=(768,))([input_ids, attention_mask])
    text_dense = Dense(128, activation='relu')(bert_output)
    combined = Concatenate()([num_dense, text_dense])
    combined = Dropout(0.6)(combined)
    combined = Dense(128, activation='relu')(combined)
    combined = Dropout(0.4)(combined)
    combined = Dense(64, activation='relu')(combined)
    output = Dense(1, activation='sigmoid')(combined)
    model = Model(inputs=[num_input, input_ids, attention_mask], outputs=output)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
                  loss='binary_crossentropy',
                  metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])
    return model

lstm_model = build_hybrid_model(len(num_features), finbert)
lstm_model.summary()

class_counts = np.bincount(y_train.astype(int))
class_weights = {0: len(y_train) / (2 * class_counts[0]), 1: len(y_train) / (2 * class_counts[1])}

callbacks = [
    EarlyStopping(monitor='val_auc', patience=5, mode='max', restore_best_weights=True),
    ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=3, min_lr=1e-6),
    ModelCheckpoint(os.path.join(drive_model_path, 'best_model.keras'), monitor='val_auc', save_best_only=True, mode='max'),
    WarmUpScheduler(warmup_epochs=3, initial_lr=1e-5, final_lr=1e-4)
]

start_time = time.time()
history_lstm = lstm_model.fit(
    {'num_input': X_train_num, 'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']},
    y_train,
    validation_data=({'num_input': X_test_num, 'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']}, y_test),
    epochs=10,
    batch_size=32,
    callbacks=callbacks,
    class_weight=class_weights
)
lstm_training_time = time.time() - start_time

lstm_model.save(os.path.join(drive_model_path, 'final_model.keras'), save_format='tf', include_optimizer=False)

# Evaluate LSTM+FinBERT
print("\n=== LSTM+FinBERT Model Evaluation ===")
y_pred_lstm = lstm_model.predict(
    {'num_input': X_test_num, 'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']}
).flatten()
lstm_report = classification_report(y_test, (y_pred_lstm > 0.5).astype(int), output_dict=True)
lstm_auc = roc_auc_score(y_test, y_pred_lstm)

# Print detailed metrics
print("Classification Report:")
print(pd.DataFrame(lstm_report).transpose())
print(f"AUC: {lstm_auc:.4f}")

# Confusion Matrix
cm_lstm = confusion_matrix(y_test, (y_pred_lstm > 0.5).astype(int))
print("\nConfusion Matrix:")
print(cm_lstm)

# Precision-Recall Curve
precision_lstm, recall_lstm, _ = precision_recall_curve(y_test, y_pred_lstm)
print("\nPrecision-Recall Curve (First 10 points):")
print(pd.DataFrame({'Precision': precision_lstm[:10], 'Recall': recall_lstm[:10]}))

X_train_rf = X_train_num.reshape(X_train_num.shape[0], -1)
X_test_rf = X_test_num.reshape(X_test_num.shape[0], -1)

rf_model = RandomForestClassifier(n_estimators=50, random_state=42)
start_time = time.time()
rf_model.fit(X_train_rf, y_train)
rf_training_time = time.time() - start_time

joblib.dump(rf_model, os.path.join(drive_model_path, 'rf_model.pkl'))

# Evaluate Random Forest
print("\n=== Random Forest Model Evaluation ===")
y_pred_rf = rf_model.predict_proba(X_test_rf)[:, 1]
rf_report = classification_report(y_test, (y_pred_rf > 0.5).astype(int), output_dict=True)
rf_auc = roc_auc_score(y_test, y_pred_rf)

# Print detailed metrics
print("Classification Report:")
print(pd.DataFrame(rf_report).transpose())
print(f"AUC: {rf_auc:.4f}")

# Confusion Matrix
cm_rf = confusion_matrix(y_test, (y_pred_rf > 0.5).astype(int))
print("\nConfusion Matrix:")
print(cm_rf)

# Precision-Recall Curve
precision_rf, recall_rf, _ = precision_recall_curve(y_test, y_pred_rf)
print("\nPrecision-Recall Curve (First 10 points):")
print(pd.DataFrame({'Precision': precision_rf[:10], 'Recall': recall_rf[:10]}))

def build_rnn_model(num_features_count, bert_model):
    num_input = Input(shape=(window_size, num_features_count), name='num_input', dtype='float32')
    num_rnn = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(128, return_sequences=True))(num_input)
    num_norm = tf.keras.layers.BatchNormalization()(num_rnn)  # Replaced LayerNormalization with BatchNormalization
    num_att = ScaledDotProductAttention()([num_norm, num_norm, num_norm])
    num_rnn2 = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(64))(num_att)
    num_dense = Dense(128, activation='relu')(num_rnn2)
    input_ids = Input(shape=(128,), dtype=tf.int32, name='input_ids')
    attention_mask = Input(shape=(128,), dtype=tf.int32, name='attention_mask')
    def call_bert(inputs):
        input_ids, attention_mask = inputs
        outputs = bert_model({'input_ids': input_ids, 'attention_mask': attention_mask})
        return outputs.last_hidden_state[:, 0, :]
    bert_output = Lambda(call_bert, output_shape=(768,))([input_ids, attention_mask])
    text_dense = Dense(128, activation='relu')(bert_output)
    combined = Concatenate()([num_dense, text_dense])
    combined = Dropout(0.6)(combined)
    combined = Dense(128, activation='relu')(combined)
    combined = Dropout(0.4)(combined)
    combined = Dense(64, activation='relu')(combined)
    output = Dense(1, activation='sigmoid')(combined)
    model = Model(inputs=[num_input, input_ids, attention_mask], outputs=output)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
                  loss='binary_crossentropy',
                  metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])
    return model

rnn_model = build_rnn_model(len(num_features), finbert)
rnn_model.summary()

callbacks_rnn = [
    EarlyStopping(monitor='val_auc', patience=5, mode='max', restore_best_weights=True),
    ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=3, min_lr=1e-6),
    ModelCheckpoint(os.path.join(drive_model_path, 'rnn_model.keras'), monitor='val_auc', save_best_only=True, mode='max'),
    WarmUpScheduler(warmup_epochs=3, initial_lr=1e-5, final_lr=1e-4)
]

start_time = time.time()
history_rnn = rnn_model.fit(
    {'num_input': X_train_num, 'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']},
    y_train,
    validation_data=({'num_input': X_test_num, 'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']}, y_test),
    epochs=10,
    batch_size=32,
    callbacks=callbacks_rnn,
    class_weight=class_weights
)
rnn_training_time = time.time() - start_time

rnn_model.save(os.path.join(drive_model_path, 'rnn_final_model.keras'), save_format='tf', include_optimizer=False)

# Evaluate RNN
print("\n=== RNN Model Evaluation ===")
y_pred_rnn = rnn_model.predict(
    {'num_input': X_test_num, 'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']}
).flatten()
rnn_report = classification_report(y_test, (y_pred_rnn > 0.5).astype(int), output_dict=True)
rnn_auc = roc_auc_score(y_test, y_pred_rnn)

# Print detailed metrics
print("Classification Report:")
print(pd.DataFrame(rnn_report).transpose())
print(f"AUC: {rnn_auc:.4f}")

# Confusion Matrix
cm_rnn = confusion_matrix(y_test, (y_pred_rnn > 0.5).astype(int))
print("\nConfusion Matrix:")
print(cm_rnn)

# Precision-Recall Curve
precision_rnn, recall_rnn, _ = precision_recall_curve(y_test, y_pred_rnn)
print("\nPrecision-Recall Curve (First 10 points):")
print(pd.DataFrame({'Precision': precision_rnn[:10], 'Recall': recall_rnn[:10]}))

# Model Comparison Table (as before)
comparison_data = {
    'Model': ['LSTM+FinBERT', 'Random Forest', 'RNN'],
    'Accuracy': [lstm_report['accuracy'], rf_report['accuracy'], rnn_report['accuracy']],
    'AUC': [lstm_auc, rf_auc, rnn_auc],
    'F1-Score (Class 1)': [lstm_report['1']['f1-score'], rf_report['1']['f1-score'], rnn_report['1']['f1-score']],
    'Training Time (s)': [lstm_training_time, rf_training_time, rnn_training_time]
}
comparison_df = pd.DataFrame(comparison_data)
comparison_df.to_csv(os.path.join(drive_model_path, 'model_comparison.csv'))

print("\nModel Comparison:")
print(comparison_df)

np.save(os.path.join(drive_model_path, 'X_test_num.npy'), X_test_num)
np.save(os.path.join(drive_model_path, 'y_test.npy'), y_test)
np.save(os.path.join(drive_model_path, 'test_encodings_input_ids.npy'), test_encodings['input_ids'].numpy())
np.save(os.path.join(drive_model_path, 'test_encodings_attention_mask.npy'), test_encodings['attention_mask'].numpy())

print(f"All models and data saved to Google Drive at: {drive_model_path}")

# Mount Google Drive

from google.colab import drive
import os
drive.mount('/content/drive')
drive_model_path = '/content/drive/My Drive/StockPrediction_Model'
if not os.path.exists(drive_model_path):
    os.makedirs(drive_model_path)

!cp -r /content/drive/My\ Drive/StockPrediction_Model /content/drive/My\ Drive/StockPrediction_Model_Backup

# Configure GPU memory growth before any TensorFlow imports
import tensorflow as tf
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(f"GPU configuration error: {e}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, precision_recall_curve
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, LSTM, Concatenate, Dropout, Bidirectional, Lambda, LayerNormalization
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from transformers import AutoTokenizer, TFAutoModel
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.layers import Layer
from tensorflow.keras.callbacks import Callback
import os
from google.colab import drive

# Mount Google Drive to save the model
drive.mount('/content/drive')
# Define a folder in Google Drive to save the model (create it manually in Drive if it doesn't exist)
drive_model_path = '/content/drive/My Drive/StockPrediction_Model'
if not os.path.exists(drive_model_path):
    os.makedirs(drive_model_path)

# Custom Attention Layer
class ScaledDotProductAttention(Layer):
    def __init__(self, **kwargs):
        super(ScaledDotProductAttention, self).__init__(**kwargs)

    def build(self, input_shape):
        super(ScaledDotProductAttention, self).build(input_shape)

    def call(self, inputs):
        query, key, value = inputs
        d_k = tf.cast(tf.shape(key)[-1], tf.float32)
        scores = tf.matmul(query, key, transpose_b=True) / tf.sqrt(d_k)
        weights = tf.nn.softmax(scores, axis=-1)
        output = tf.matmul(weights, value)
        return output

# Custom Learning Rate Scheduler with Warmup
class WarmUpScheduler(Callback):
    def __init__(self, warmup_epochs, initial_lr, final_lr):
        super(WarmUpScheduler, self).__init__()
        self.warmup_epochs = warmup_epochs
        self.initial_lr = initial_lr
        self.final_lr = final_lr

    def on_epoch_begin(self, epoch, logs=None):
        if epoch < self.warmup_epochs:
            lr = self.initial_lr + (self.final_lr - self.initial_lr) * (epoch / self.warmup_epochs)
            self.model.optimizer.learning_rate.assign(lr)

# Load and preprocess data
combined_df = pd.read_csv('Combined_News_DJIA.csv', parse_dates=['Date'])
djia_df = pd.read_csv('upload_DJIA_table.csv', parse_dates=['Date'])
reddit_df = pd.read_csv('RedditNews.csv', parse_dates=['Date'])

full_df = pd.merge(combined_df, djia_df, on='Date', how='inner')

# Enhanced technical indicators function
def add_technical_indicators(df):
    """Add technical indicators to the stock data."""
    # Price transformations
    df['Log_Return'] = np.log(df['Adj Close'] / df['Adj Close'].shift(1))
    df['MA_10'] = df['Adj Close'].rolling(window=10).mean()
    df['MA_20'] = df['Adj Close'].rolling(window=20).mean()
    df['MA_50'] = df['Adj Close'].rolling(window=50).mean()
    df['MA_200'] = df['Adj Close'].rolling(window=200).mean()

    # Volatility
    df['Volatility_20'] = df['Adj Close'].rolling(window=20).std()
    df['Volatility_50'] = df['Adj Close'].rolling(window=50).std()

    # Momentum indicators
    delta = df['Adj Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df['RSI'] = 100 - (100 / (1 + rs))

    # MACD
    exp12 = df['Adj Close'].ewm(span=12, adjust=False).mean()
    exp26 = df['Adj Close'].ewm(span=26, adjust=False).mean()
    df['MACD'] = exp12 - exp26
    df['Signal_Line'] = df['MACD'].ewm(span=9, adjust=False).mean()
    df['MACD_Hist'] = df['MACD'] - df['Signal_Line']

    # Bollinger Bands
    df['Upper_Band'] = df['MA_20'] + (2 * df['Volatility_20'])
    df['Lower_Band'] = df['MA_20'] - (2 * df['Volatility_20'])
    df['Band_Width'] = df['Upper_Band'] - df['Lower_Band']

    # Stochastic Oscillator
    low_14 = df['Low'].rolling(window=14).min()
    high_14 = df['High'].rolling(window=14).max()
    df['Stochastic_K'] = 100 * ((df['Close'] - low_14) / (high_14 - low_14))
    df['Stochastic_D'] = df['Stochastic_K'].rolling(window=3).mean()

    return df.dropna()

full_df = add_technical_indicators(full_df)

# Enhanced text preprocessing with sentiment analysis
def get_sentiment_scores(texts, tokenizer, model, batch_size=16):
    """Extract sentiment scores using FinBERT in batches with a classification head."""
    sentiment_scores = []
    try:
        # Define classification head
        classification_head = Dense(3, activation='softmax', name='sentiment_classifier')

        for i in range(0, len(texts), batch_size):
            tf.keras.backend.clear_session()  # Clear previous session to manage memory
            batch_texts = texts[i:i + batch_size]
            encodings = tokenizer(batch_texts, truncation=True, padding='max_length', max_length=128, return_tensors='tf')
            outputs = model(encodings)
            cls_embeddings = outputs.last_hidden_state[:, 0, :]  # Extract [CLS] token embedding
            logits = classification_head(cls_embeddings)  # Apply classification head
            probs = tf.nn.softmax(logits, axis=-1)  # Convert to probabilities
            sentiment_scores.append(probs.numpy())
        return np.vstack(sentiment_scores)  # [positive, negative, neutral]
    except Exception as e:
        print(f"Error in sentiment score computation: {e}")
        raise

full_df['Combined_News'] = full_df.filter(regex='Top\d+').apply(
    lambda x: ' '.join(x.str.lower()
                      .replace(r'[^\w\s]', '', regex=True)
                      .replace(r'\d+', '', regex=True)), axis=1)

# Initialize FinBERT for sentiment analysis
tokenizer = AutoTokenizer.from_pretrained("yiyanghkust/finbert-tone")
finbert = TFAutoModel.from_pretrained("yiyanghkust/finbert-tone")

# Compute sentiment scores
sentiment_scores = get_sentiment_scores(full_df['Combined_News'].tolist(), tokenizer, finbert)
full_df['Sentiment_Positive'] = sentiment_scores[:, 0]
full_df['Sentiment_Negative'] = sentiment_scores[:, 1]
full_df['Sentiment_Neutral'] = sentiment_scores[:, 2]

# Train-Test Split (chronological)
train_df = full_df[full_df['Date'] < '2015-01-01']
test_df = full_df[full_df['Date'] >= '2015-01-01']

# Numerical Features
num_features = ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close',
                'Log_Return', 'MA_10', 'MA_20', 'MA_50', 'MA_200',
                'Volatility_20', 'Volatility_50', 'RSI',
                'MACD', 'Signal_Line', 'MACD_Hist',
                'Upper_Band', 'Lower_Band', 'Band_Width',
                'Stochastic_K', 'Stochastic_D',
                'Sentiment_Positive', 'Sentiment_Negative', 'Sentiment_Neutral']

# Normalize
train_mean = train_df[num_features].mean()
train_std = train_df[num_features].std()
train_num = (train_df[num_features] - train_mean) / train_std
test_num = (test_df[num_features] - train_mean) / train_std

# Data Augmentation using Random Perturbations
def augment_numerical_data(data, n_samples, noise_factor=0.1):
    """Generate synthetic numerical data by adding random perturbations."""
    if not isinstance(data, np.ndarray):
        data = np.array(data)

    # Calculate standard deviation for each feature to scale noise
    std = np.std(data, axis=0)
    synthetic_data = np.zeros((n_samples, data.shape[1]))

    # Randomly sample indices from original data
    indices = np.random.choice(data.shape[0], size=n_samples, replace=True)
    for i in range(n_samples):
        # Add Gaussian noise scaled by feature std and noise factor
        noise = np.random.normal(0, noise_factor * std, size=data.shape[1])
        synthetic_data[i] = data[indices[i]] + noise

    return synthetic_data, indices

# Augment training data (numerical and text)
n_samples = len(train_num)
synthetic_data, selected_indices = augment_numerical_data(train_num.values, n_samples=n_samples)
augmented_train_num = np.vstack([train_num.values, synthetic_data])
train_texts = train_df['Combined_News'].tolist()
# Duplicate text data to match augmented numerical data
augmented_train_texts = train_texts + [train_texts[i] for i in selected_indices]
augmented_y_train = np.hstack([train_df['Label'].values, train_df['Label'].iloc[selected_indices].values])

# Sequence creation
window_size = 20
def create_sequences(data, labels, texts, tokenizer, window_size=20):
    """Create time-series sequences for numerical and text data."""
    sequences = []
    label_seq = []
    text_seq = []
    for i in range(len(data) - window_size):
        sequences.append(data[i:i+window_size])
        label_seq.append(labels[i+window_size])
        text_seq.append(texts[i+window_size])
    sequences = np.array(sequences)
    label_seq = np.array(label_seq)
    # Tokenize text sequences
    encodings = tokenizer(text_seq, truncation=True, padding='max_length', max_length=128, return_tensors='tf')
    return sequences, label_seq, encodings

# Create sequences for training and test data
X_train_num, y_train, train_encodings = create_sequences(augmented_train_num, augmented_y_train, augmented_train_texts, tokenizer, window_size)
X_test_num, y_test, test_encodings = create_sequences(test_num.values, test_df['Label'].values, test_df['Combined_News'].tolist(), tokenizer, window_size)

# Validate sample sizes
n_train_samples = X_train_num.shape[0]
if not (n_train_samples == train_encodings['input_ids'].shape[0] == train_encodings['attention_mask'].shape[0] == y_train.shape[0]):
    raise ValueError(f"Training data cardinality mismatch: X_train_num={n_train_samples}, "
                     f"input_ids={train_encodings['input_ids'].shape[0]}, "
                     f"attention_mask={train_encodings['attention_mask'].shape[0]}, "
                     f"y_train={y_train.shape[0]}")

n_test_samples = X_test_num.shape[0]
if not (n_test_samples == test_encodings['input_ids'].shape[0] == test_encodings['attention_mask'].shape[0] == y_test.shape[0]):
    raise ValueError(f"Test data cardinality mismatch: X_test_num={n_test_samples}, "
                     f"input_ids={test_encodings['input_ids'].shape[0]}, "
                     f"attention_mask={test_encodings['attention_mask'].shape[0]}, "
                     f"y_test={y_test.shape[0]}")

# Enhanced Model Architecture
def build_hybrid_model(num_features_count, bert_model):
    """Build the hybrid LSTM-FinBERT model."""
    # Numerical branch with attention
    num_input = Input(shape=(window_size, num_features_count), name='num_input')
    num_lstm = Bidirectional(LSTM(128, return_sequences=True))(num_input)
    num_norm = LayerNormalization()(num_lstm)
    num_att = ScaledDotProductAttention()([num_norm, num_norm, num_norm])
    num_lstm2 = Bidirectional(LSTM(64))(num_att)
    num_dense = Dense(128, activation='relu')(num_lstm2)

    # Text branch with FinBERT
    input_ids = Input(shape=(128,), dtype=tf.int32, name='input_ids')
    attention_mask = Input(shape=(128,), dtype=tf.int32, name='attention_mask')

    def call_bert(inputs):
        input_ids, attention_mask = inputs
        outputs = bert_model({'input_ids': input_ids, 'attention_mask': attention_mask})
        return outputs.last_hidden_state[:, 0, :]  # Use [CLS] token representation

    bert_output = Lambda(call_bert, output_shape=(768,))([input_ids, attention_mask])
    text_dense = Dense(128, activation='relu')(bert_output)

    # Combined with deeper architecture
    combined = Concatenate()([num_dense, text_dense])
    combined = Dropout(0.6)(combined)
    combined = Dense(128, activation='relu')(combined)
    combined = Dropout(0.4)(combined)
    combined = Dense(64, activation='relu')(combined)
    output = Dense(1, activation='sigmoid')(combined)

    model = Model(inputs=[num_input, input_ids, attention_mask], outputs=output)

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
        loss='binary_crossentropy',
        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]
    )

    return model

model = build_hybrid_model(len(num_features), finbert)
model.summary()

# Compute class weights
class_counts = np.bincount(y_train.astype(int))
class_weights = {0: len(y_train) / (2 * class_counts[0]), 1: len(y_train) / (2 * class_counts[1])}

# Enhanced callbacks
callbacks = [
    EarlyStopping(monitor='val_auc', patience=15, mode='max', restore_best_weights=True),
    ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=5, min_lr=1e-6),
    ModelCheckpoint('best_model.keras', monitor='val_auc', save_best_only=True, mode='max'),
    WarmUpScheduler(warmup_epochs=5, initial_lr=1e-5, final_lr=1e-4)
]

# Train the model
history = model.fit(
    {'num_input': X_train_num,
     'input_ids': train_encodings['input_ids'],
     'attention_mask': train_encodings['attention_mask']},
    y_train,
    validation_data=(
        {'num_input': X_test_num,
         'input_ids': test_encodings['input_ids'],
         'attention_mask': test_encodings['attention_mask']},
        y_test
    ),
    epochs=30,
    batch_size=64,
    callbacks=callbacks,
    class_weight=class_weights
)

# Save the final trained model
model.save('final_model.keras')

# Copy the saved models to Google Drive
import shutil
shutil.copy('best_model.keras', os.path.join(drive_model_path, 'best_model.keras'))
shutil.copy('final_model.keras', os.path.join(drive_model_path, 'final_model.keras'))
print(f"Models saved to Google Drive at: {drive_model_path}")

# Enhanced Evaluation
y_pred = model.predict(
    {'num_input': X_test_num,
     'input_ids': test_encodings['input_ids'],
     'attention_mask': test_encodings['attention_mask']}
).flatten()

print("Classification Report:")
print(classification_report(y_test, (y_pred > 0.5).astype(int)))
print(f"AUC Score: {roc_auc_score(y_test, y_pred):.4f}")

# Confusion Matrix
cm = confusion_matrix(y_test, (y_pred > 0.5).astype(int))
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Precision-Recall Curve
precision, recall, _ = precision_recall_curve(y_test, y_pred)
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, label='Precision-Recall Curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.show()

# Plot training history
plt.figure(figsize=(15, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['auc'], label='Train AUC')
plt.plot(history.history['val_auc'], label='Validation AUC')
plt.title('Model AUC Over Epochs')
plt.ylabel('AUC')
plt.xlabel('Epoch')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss Over Epochs')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()
plt.show()

# Generate Project Report
def generate_report(y_test, y_pred, history):
    """Generate a summary report of the model's performance."""
    report = f"""
    Stock Price Prediction Model Report
    ==================================
    Dataset Statistics:
    - Training samples: {len(X_train_num)}
    - Test samples: {len(X_test_num)}
    - Features: {len(num_features)}
    - Window size: {window_size}

    Model Performance:
    - AUC Score: {roc_auc_score(y_test, y_pred):.4f}
    - Classification Report:
    {classification_report(y_test, (y_pred > 0.5).astype(int))}

    Training Summary:
    - Best Validation AUC: {max(history.history['val_auc']):.4f}
    - Best Validation Loss: {min(history.history['val_loss']):.4f}
    """
    print(report)
    return report

report = generate_report(y_test, y_pred, history)